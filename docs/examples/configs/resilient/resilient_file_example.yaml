# Resilient File/Storage Extraction Example
#
# This example shows how to configure Bronze extraction from cloud storage
# (S3, Azure Blob) with resilience patterns for transient storage errors.
#
# Use this as a starting point for file sources that:
# - Are stored in cloud storage with occasional timeouts
# - Experience rate limiting on list/get operations
# - Need to handle partial failures gracefully
#
# See: docs/guides/error_handling.md for full documentation

# ============================================================================
# Platform Configuration (S3 Storage)
# ============================================================================
platform:
  storage:
    source:
      backend: s3
      bucket: my-data-lake
      prefix: raw/sales/

    bronze:
      backend: s3
      bucket: my-data-lake
      prefix: bronze/

    silver:
      backend: s3
      bucket: my-data-lake
      prefix: silver/

  # AWS credentials (prefer IAM roles in production)
  # These can also be set via AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY env vars
  aws:
    region: us-east-1
    # access_key_id: "${AWS_ACCESS_KEY_ID}"
    # secret_access_key: "${AWS_SECRET_ACCESS_KEY}"

# ============================================================================
# Bronze Configuration
# ============================================================================
bronze:
  source_type: file
  system: sales_data
  table: transactions

  # Output configuration
  options:
    format: parquet

# ============================================================================
# Source Configuration
# ============================================================================
source:
  type: file

  # File pattern to extract (supports glob patterns)
  path: "s3://my-data-lake/raw/sales/transactions/*.csv"

  # File format settings
  format: csv
  csv_options:
    delimiter: ","
    header: true
    encoding: utf-8

  # Schema inference (or provide explicit schema)
  infer_schema: true

# ============================================================================
# Run Configuration
# ============================================================================
run:
  # Load pattern for this extraction
  load_pattern: snapshot

  # Process files in parallel for better throughput
  parallel_workers: 4

  # Continue if some files fail (useful for large file sets)
  continue_on_error: true
  max_error_rate: 0.10  # Fail if more than 10% of files error

# ============================================================================
# Silver Configuration (optional)
# ============================================================================
silver:
  entity_kind: event
  domain: sales
  entity: transactions
  version: 1

  natural_keys:
    - transaction_id

  timestamp_column: transaction_date

# ============================================================================
# Resilience Notes
# ============================================================================
#
# Storage backend resilience (S3/Azure):
# - Separate circuit breakers for: upload, download, list, delete
# - Each operation type fails independently
# - One failed upload doesn't block downloads
#
# S3-specific retry behavior:
# - ClientError (access denied, not found) - NOT retryable
# - Connection errors - retryable
# - Throttling (503 SlowDown) - retryable with backoff
# - Socket timeouts - retryable
#
# Azure-specific retry behavior:
# - ResourceNotFoundError - NOT retryable
# - Connection errors - retryable
# - Throttling - retryable with backoff
#
# Built-in defaults:
# - Max attempts: 5
# - Base delay: 0.5s
# - Max delay: 8.0s
# - Backoff: 2x exponential
#
# Best practices for cloud storage:
# 1. Use appropriate IAM permissions (least privilege)
# 2. Enable server-side encryption
# 3. Use lifecycle policies for cleanup
# 4. Monitor storage costs and access patterns
